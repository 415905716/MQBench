[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Training
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: True / Pot scale: False / Extra kwargs: {'p': 2.4}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: FixedFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {'p': 2.4}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Set reshape_1 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant reshape_1_post_act_fake_quantizer
begin load datset
finish load datset
begin calibration now!
[MQBENCH] INFO: Enable observer and Disable quantize for act_fake_quant
[rank 0000]-[INFO]-[4256]-[2022-02-23 11:36:02]-[/spring/src/linklink/src/core.cc:219]: linklink init: world_size=1, rank=(0,0), device_num=1, thread_pool=1, buffer_pool=4
[MQBENCH] INFO: Enable observer and Disable quantize for weight_fake_quant
begin advanced PTQ now!
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare layer reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [conv1, bn1, relu_1, maxpool, maxpool_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (conv1): Conv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
      (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
    )
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (maxpool_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0821], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.2313255071640015 ch_axis=-1 pot=False)
  )
)

def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu_1 = self.relu(bn1);  bn1 = None
    maxpool = self.maxpool(relu_1);  relu_1 = None
    maxpool_post_act_fake_quantizer = self.maxpool_post_act_fake_quantizer(maxpool);  maxpool = None
    return maxpool_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for maxpool_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.031 (rec:0.031, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.033 (rec:0.033, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.031 (rec:0.031, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.032 (rec:0.032, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.033 (rec:0.033, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.031 (rec:0.031, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.033 (rec:0.033, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	87.108 (rec:0.035, round:87.073)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	38.987 (rec:0.030, round:38.956)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	36.192 (rec:0.034, round:36.158)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	34.322 (rec:0.031, round:34.291)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	32.832 (rec:0.031, round:32.801)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	31.492 (rec:0.030, round:31.462)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	30.251 (rec:0.032, round:30.219)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	29.008 (rec:0.033, round:28.975)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	27.819 (rec:0.034, round:27.785)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	26.768 (rec:0.035, round:26.733)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	25.599 (rec:0.032, round:25.567)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	24.174 (rec:0.033, round:24.141)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	22.769 (rec:0.033, round:22.736)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	21.600 (rec:0.033, round:21.567)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	20.367 (rec:0.031, round:20.336)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	18.921 (rec:0.032, round:18.889)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	17.498 (rec:0.030, round:17.468)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	16.164 (rec:0.031, round:16.133)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	14.798 (rec:0.032, round:14.766)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	13.314 (rec:0.031, round:13.283)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	11.647 (rec:0.032, round:11.615)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	9.891 (rec:0.032, round:9.859)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	8.041 (rec:0.032, round:8.009)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	6.062 (rec:0.033, round:6.029)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	4.154 (rec:0.036, round:4.118)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	2.604 (rec:0.034, round:2.570)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1.203 (rec:0.031, round:1.172)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	0.490 (rec:0.032, round:0.459)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	0.213 (rec:0.035, round:0.178)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.074 (rec:0.031, round:0.043)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.037 (rec:0.031, round:0.006)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.032 (rec:0.032, round:0.000)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.032 (rec:0.032, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer1.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_conv1, layer1_0_bn1, layer1_0_relu1, layer1_0_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer1_0_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8902527093887329 ch_axis=-1 pot=False)
  )
  (layer1): Module(
    (0): Module(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, maxpool_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(maxpool_post_act_fake_quantizer);  maxpool_post_act_fake_quantizer = None
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu1 = getattr(self.layer1, "0").relu1(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu1_post_act_fake_quantizer = self.layer1_0_relu1_post_act_fake_quantizer(layer1_0_relu1);  layer1_0_relu1 = None
    return layer1_0_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.165 (rec:0.165, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.156 (rec:0.156, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.144 (rec:0.144, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.141 (rec:0.141, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.138 (rec:0.138, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.136 (rec:0.136, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.141 (rec:0.141, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	297.347 (rec:0.136, round:297.211)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	126.383 (rec:0.137, round:126.246)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	115.015 (rec:0.126, round:114.889)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	108.397 (rec:0.127, round:108.271)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	103.128 (rec:0.121, round:103.007)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	98.182 (rec:0.119, round:98.063)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	93.496 (rec:0.119, round:93.377)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	88.910 (rec:0.121, round:88.789)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	84.442 (rec:0.124, round:84.317)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	80.642 (rec:0.124, round:80.518)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	76.261 (rec:0.124, round:76.136)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	72.121 (rec:0.120, round:72.001)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	68.092 (rec:0.126, round:67.966)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	64.179 (rec:0.127, round:64.051)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	60.284 (rec:0.127, round:60.157)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	56.169 (rec:0.128, round:56.041)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	51.965 (rec:0.123, round:51.842)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	47.289 (rec:0.119, round:47.170)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	42.901 (rec:0.127, round:42.774)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	37.763 (rec:0.124, round:37.639)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	32.305 (rec:0.121, round:32.184)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	26.943 (rec:0.129, round:26.813)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	21.678 (rec:0.126, round:21.552)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	16.437 (rec:0.134, round:16.303)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	11.659 (rec:0.135, round:11.524)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	7.420 (rec:0.139, round:7.281)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	3.875 (rec:0.138, round:3.737)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1.556 (rec:0.142, round:1.414)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	0.620 (rec:0.138, round:0.482)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.224 (rec:0.145, round:0.079)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.143 (rec:0.141, round:0.002)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.140 (rec:0.140, round:0.000)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.146 (rec:0.146, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer1.0.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_conv2, layer1_0_bn2]
[MQBENCH] INFO: GraphModule(
  (layer1): Module(
    (0): Module(
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer1_0_relu1_post_act_fake_quantizer):
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu1_post_act_fake_quantizer);  layer1_0_relu1_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    return layer1_0_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.674 (rec:0.674, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.553 (rec:0.553, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.533 (rec:0.533, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.506 (rec:0.506, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.533 (rec:0.533, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.489 (rec:0.489, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.418 (rec:0.418, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	326.214 (rec:0.408, round:325.807)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	189.104 (rec:0.424, round:188.680)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	173.369 (rec:0.425, round:172.944)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	163.623 (rec:0.444, round:163.178)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	154.886 (rec:0.417, round:154.469)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	145.762 (rec:0.436, round:145.326)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	137.617 (rec:0.406, round:137.212)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	130.230 (rec:0.418, round:129.813)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	123.243 (rec:0.412, round:122.831)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	116.107 (rec:0.417, round:115.690)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	108.963 (rec:0.423, round:108.540)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	102.934 (rec:0.403, round:102.531)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	96.857 (rec:0.404, round:96.453)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	91.109 (rec:0.436, round:90.673)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	84.977 (rec:0.410, round:84.567)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	78.901 (rec:0.412, round:78.489)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	72.765 (rec:0.425, round:72.339)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	67.124 (rec:0.426, round:66.699)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	61.245 (rec:0.423, round:60.822)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	55.292 (rec:0.430, round:54.862)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	48.948 (rec:0.434, round:48.514)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	42.460 (rec:0.425, round:42.036)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	35.585 (rec:0.457, round:35.129)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	28.952 (rec:0.438, round:28.513)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	22.505 (rec:0.449, round:22.056)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	16.777 (rec:0.441, round:16.335)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	11.294 (rec:0.499, round:10.795)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	6.499 (rec:0.447, round:6.052)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	2.967 (rec:0.466, round:2.501)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.185 (rec:0.474, round:0.711)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.645 (rec:0.519, round:0.125)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.477 (rec:0.467, round:0.010)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.467 (rec:0.467, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer1.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_conv1, layer1_1_bn1, layer1_1_relu1, layer1_1_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer1_1_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0625], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9378153085708618 ch_axis=-1 pot=False)
  )
  (layer1): Module(
    (1): Module(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer1_0_relu2_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu2_post_act_fake_quantizer);  layer1_0_relu2_post_act_fake_quantizer = None
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu1 = getattr(self.layer1, "1").relu1(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu1_post_act_fake_quantizer = self.layer1_1_relu1_post_act_fake_quantizer(layer1_1_relu1);  layer1_1_relu1 = None
    return layer1_1_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.134 (rec:0.134, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.122 (rec:0.122, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.122 (rec:0.122, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.118 (rec:0.118, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.110 (rec:0.110, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.113 (rec:0.113, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.109 (rec:0.109, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	336.030 (rec:0.105, round:335.925)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	181.398 (rec:0.106, round:181.292)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	166.924 (rec:0.106, round:166.818)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	157.510 (rec:0.105, round:157.405)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	149.044 (rec:0.104, round:148.940)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	141.015 (rec:0.103, round:140.913)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	133.003 (rec:0.101, round:132.902)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	125.452 (rec:0.104, round:125.348)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	118.271 (rec:0.104, round:118.167)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	111.298 (rec:0.105, round:111.193)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	104.743 (rec:0.107, round:104.637)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	98.003 (rec:0.105, round:97.898)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	91.333 (rec:0.105, round:91.228)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	84.662 (rec:0.107, round:84.554)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	78.142 (rec:0.105, round:78.037)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	71.927 (rec:0.108, round:71.819)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	65.613 (rec:0.109, round:65.505)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	58.761 (rec:0.106, round:58.655)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	52.561 (rec:0.105, round:52.456)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	46.322 (rec:0.110, round:46.211)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	39.775 (rec:0.108, round:39.667)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	33.366 (rec:0.112, round:33.254)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	26.215 (rec:0.110, round:26.105)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	19.899 (rec:0.112, round:19.787)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	14.067 (rec:0.112, round:13.955)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	8.809 (rec:0.113, round:8.696)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4.545 (rec:0.117, round:4.428)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	1.775 (rec:0.117, round:1.658)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	0.579 (rec:0.115, round:0.464)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.227 (rec:0.117, round:0.111)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.137 (rec:0.117, round:0.020)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.117 (rec:0.115, round:0.001)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.118 (rec:0.118, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer1.1.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_conv2, layer1_1_bn2]
[MQBENCH] INFO: GraphModule(
  (layer1): Module(
    (1): Module(
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer1_1_relu1_post_act_fake_quantizer):
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu1_post_act_fake_quantizer);  layer1_1_relu1_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    return layer1_1_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1.084 (rec:1.084, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.986 (rec:0.986, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.947 (rec:0.947, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.923 (rec:0.923, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.916 (rec:0.916, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.900 (rec:0.900, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.860 (rec:0.860, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	327.150 (rec:0.890, round:326.260)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	191.947 (rec:0.889, round:191.059)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	177.427 (rec:0.941, round:176.486)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	168.088 (rec:0.911, round:167.178)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	160.051 (rec:0.907, round:159.145)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	152.616 (rec:0.911, round:151.704)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	145.790 (rec:0.889, round:144.901)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	139.828 (rec:0.888, round:138.940)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	133.795 (rec:0.891, round:132.904)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	128.066 (rec:0.896, round:127.170)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	122.353 (rec:0.915, round:121.439)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	116.684 (rec:0.925, round:115.760)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	111.465 (rec:0.896, round:110.569)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	106.244 (rec:0.895, round:105.349)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	100.443 (rec:0.900, round:99.543)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	94.346 (rec:0.905, round:93.441)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	87.704 (rec:0.905, round:86.799)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	81.475 (rec:0.901, round:80.574)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	74.953 (rec:0.934, round:74.019)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	67.905 (rec:0.971, round:66.934)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	60.106 (rec:0.923, round:59.182)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	51.881 (rec:0.949, round:50.933)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	43.452 (rec:0.949, round:42.503)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	34.857 (rec:0.964, round:33.894)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	26.435 (rec:0.951, round:25.485)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	18.762 (rec:0.981, round:17.781)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	12.113 (rec:0.983, round:11.130)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	6.735 (rec:0.975, round:5.760)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3.306 (rec:0.980, round:2.326)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.773 (rec:1.010, round:0.763)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.130 (rec:1.004, round:0.125)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1.010 (rec:0.997, round:0.012)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1.023 (rec:1.018, round:0.005)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer2.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_conv1, layer2_0_bn1, layer2_0_relu1, layer2_0_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer2_0_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9161999225616455 ch_axis=-1 pot=False)
  )
  (layer2): Module(
    (0): Module(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer1_1_relu2_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_1_relu2_post_act_fake_quantizer);  layer1_1_relu2_post_act_fake_quantizer = None
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_relu1 = getattr(self.layer2, "0").relu1(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu1_post_act_fake_quantizer = self.layer2_0_relu1_post_act_fake_quantizer(layer2_0_relu1);  layer2_0_relu1 = None
    return layer2_0_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.447 (rec:0.447, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.416 (rec:0.416, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.411 (rec:0.411, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.406 (rec:0.406, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.381 (rec:0.381, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.365 (rec:0.365, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.380 (rec:0.380, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	666.580 (rec:0.369, round:666.211)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	355.646 (rec:0.370, round:355.276)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	326.612 (rec:0.357, round:326.255)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	306.955 (rec:0.376, round:306.579)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	289.934 (rec:0.375, round:289.559)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	274.441 (rec:0.368, round:274.072)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	259.289 (rec:0.375, round:258.914)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	245.321 (rec:0.372, round:244.949)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	232.091 (rec:0.372, round:231.719)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	219.206 (rec:0.355, round:218.851)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	206.927 (rec:0.380, round:206.547)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	194.528 (rec:0.386, round:194.142)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	182.529 (rec:0.377, round:182.152)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	170.352 (rec:0.377, round:169.976)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	158.324 (rec:0.369, round:157.955)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	146.340 (rec:0.370, round:145.970)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	134.023 (rec:0.380, round:133.643)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	121.323 (rec:0.372, round:120.951)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	109.300 (rec:0.381, round:108.919)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	96.236 (rec:0.374, round:95.862)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	83.469 (rec:0.395, round:83.073)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	70.466 (rec:0.386, round:70.080)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	56.799 (rec:0.386, round:56.413)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	43.408 (rec:0.382, round:43.026)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	31.018 (rec:0.385, round:30.632)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	20.036 (rec:0.375, round:19.660)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	10.784 (rec:0.394, round:10.390)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4.474 (rec:0.391, round:4.083)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1.240 (rec:0.394, round:0.846)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.526 (rec:0.398, round:0.129)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.430 (rec:0.398, round:0.032)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.411 (rec:0.401, round:0.010)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.411 (rec:0.409, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer2.0.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_conv2, layer2_0_bn2]
[MQBENCH] INFO: GraphModule(
  (layer2): Module(
    (0): Module(
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer2_0_relu1_post_act_fake_quantizer):
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu1_post_act_fake_quantizer);  layer2_0_relu1_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    return layer2_0_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1.298 (rec:1.298, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1.210 (rec:1.210, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1.120 (rec:1.120, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1.119 (rec:1.119, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1.096 (rec:1.096, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1.080 (rec:1.080, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1.050 (rec:1.050, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1315.438 (rec:1.042, round:1314.396)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	705.948 (rec:1.051, round:704.897)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	649.450 (rec:1.060, round:648.389)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	611.765 (rec:1.064, round:610.701)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	580.317 (rec:1.053, round:579.265)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	552.032 (rec:1.058, round:550.974)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	525.700 (rec:1.059, round:524.641)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	501.486 (rec:1.087, round:500.399)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	476.972 (rec:1.061, round:475.911)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	454.084 (rec:1.073, round:453.011)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	432.797 (rec:1.084, round:431.713)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	411.548 (rec:1.054, round:410.494)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	390.179 (rec:1.082, round:389.097)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	369.378 (rec:1.086, round:368.293)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	347.492 (rec:1.074, round:346.418)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	324.610 (rec:1.139, round:323.471)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	300.550 (rec:1.079, round:299.471)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	277.044 (rec:1.102, round:275.942)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	252.790 (rec:1.125, round:251.665)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	227.472 (rec:1.162, round:226.310)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	200.598 (rec:1.099, round:199.499)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	173.402 (rec:1.116, round:172.286)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	143.912 (rec:1.125, round:142.786)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	112.928 (rec:1.165, round:111.763)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	82.778 (rec:1.110, round:81.668)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	55.570 (rec:1.201, round:54.369)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	32.501 (rec:1.173, round:31.329)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	15.313 (rec:1.178, round:14.135)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	5.229 (rec:1.196, round:4.033)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.907 (rec:1.197, round:0.710)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.305 (rec:1.230, round:0.075)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1.194 (rec:1.191, round:0.004)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1.239 (rec:1.239, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer2.0.downsample.0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_downsample_0, layer2_0_downsample_1]
[MQBENCH] INFO: GraphModule(
  (layer2): Module(
    (0): Module(
      (downsample): Module(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer1_1_relu2_post_act_fake_quantizer):
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_1_relu2_post_act_fake_quantizer);  layer1_1_relu2_post_act_fake_quantizer = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    return layer2_0_downsample_1
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.345 (rec:0.345, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.342 (rec:0.342, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.333 (rec:0.333, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.326 (rec:0.326, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.332 (rec:0.332, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.337 (rec:0.337, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.328 (rec:0.328, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	72.295 (rec:0.336, round:71.959)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	40.546 (rec:0.332, round:40.214)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	37.544 (rec:0.364, round:37.179)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	35.678 (rec:0.336, round:35.343)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	34.470 (rec:0.360, round:34.111)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	33.137 (rec:0.350, round:32.786)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	31.748 (rec:0.356, round:31.392)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	30.751 (rec:0.362, round:30.390)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	29.790 (rec:0.352, round:29.438)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	28.800 (rec:0.354, round:28.447)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	27.765 (rec:0.351, round:27.414)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	26.681 (rec:0.366, round:26.315)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	25.629 (rec:0.366, round:25.263)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	24.661 (rec:0.361, round:24.300)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	23.474 (rec:0.372, round:23.102)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	22.183 (rec:0.365, round:21.818)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	21.006 (rec:0.368, round:20.639)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	19.709 (rec:0.378, round:19.331)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	18.216 (rec:0.390, round:17.826)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	16.696 (rec:0.379, round:16.317)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	14.946 (rec:0.383, round:14.563)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	13.152 (rec:0.413, round:12.739)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	11.304 (rec:0.403, round:10.900)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	9.503 (rec:0.410, round:9.093)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	7.757 (rec:0.422, round:7.335)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	5.800 (rec:0.448, round:5.352)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4.035 (rec:0.435, round:3.600)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2.661 (rec:0.451, round:2.210)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1.641 (rec:0.477, round:1.164)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.962 (rec:0.474, round:0.488)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.678 (rec:0.498, round:0.180)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.564 (rec:0.500, round:0.063)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.527 (rec:0.499, round:0.028)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer2.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_conv1, layer2_1_bn1, layer2_1_relu1, layer2_1_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer2_1_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0651], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9771558046340942 ch_axis=-1 pot=False)
  )
  (layer2): Module(
    (1): Module(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer2_0_relu2_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu2_post_act_fake_quantizer);  layer2_0_relu2_post_act_fake_quantizer = None
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu1 = getattr(self.layer2, "1").relu1(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu1_post_act_fake_quantizer = self.layer2_1_relu1_post_act_fake_quantizer(layer2_1_relu1);  layer2_1_relu1 = None
    return layer2_1_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.307 (rec:0.307, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.277 (rec:0.277, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.269 (rec:0.269, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.271 (rec:0.271, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.266 (rec:0.266, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.261 (rec:0.261, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.253 (rec:0.253, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1332.039 (rec:0.257, round:1331.782)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	669.014 (rec:0.258, round:668.756)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	612.582 (rec:0.257, round:612.325)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	574.328 (rec:0.252, round:574.076)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	542.939 (rec:0.259, round:542.680)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	513.053 (rec:0.252, round:512.801)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	485.701 (rec:0.256, round:485.445)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	459.232 (rec:0.257, round:458.975)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	434.953 (rec:0.257, round:434.696)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	410.942 (rec:0.253, round:410.689)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	387.200 (rec:0.258, round:386.943)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	363.841 (rec:0.254, round:363.587)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	341.385 (rec:0.258, round:341.127)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	319.054 (rec:0.255, round:318.799)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	297.122 (rec:0.260, round:296.862)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	274.798 (rec:0.261, round:274.537)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	250.961 (rec:0.264, round:250.697)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	227.070 (rec:0.262, round:226.808)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	202.217 (rec:0.256, round:201.960)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	177.139 (rec:0.265, round:176.874)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	152.393 (rec:0.266, round:152.127)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	126.877 (rec:0.267, round:126.610)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	101.571 (rec:0.274, round:101.297)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	76.368 (rec:0.269, round:76.098)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	53.374 (rec:0.278, round:53.096)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	32.494 (rec:0.279, round:32.214)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	16.461 (rec:0.278, round:16.182)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	5.794 (rec:0.283, round:5.510)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1.705 (rec:0.284, round:1.421)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.579 (rec:0.282, round:0.297)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.333 (rec:0.271, round:0.061)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.287 (rec:0.284, round:0.003)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.279 (rec:0.279, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer2.1.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_conv2, layer2_1_bn2]
[MQBENCH] INFO: GraphModule(
  (layer2): Module(
    (1): Module(
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer2_1_relu1_post_act_fake_quantizer):
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu1_post_act_fake_quantizer);  layer2_1_relu1_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    return layer2_1_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1.723 (rec:1.723, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1.627 (rec:1.627, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1.580 (rec:1.580, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1.563 (rec:1.563, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1.542 (rec:1.542, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1.510 (rec:1.510, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1.551 (rec:1.551, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1320.215 (rec:1.536, round:1318.679)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	721.096 (rec:1.512, round:719.584)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	666.349 (rec:1.543, round:664.806)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	629.361 (rec:1.517, round:627.844)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	598.651 (rec:1.492, round:597.159)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	571.540 (rec:1.596, round:569.944)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	545.611 (rec:1.506, round:544.105)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	521.678 (rec:1.547, round:520.131)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	498.966 (rec:1.489, round:497.477)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	476.136 (rec:1.603, round:474.533)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	454.215 (rec:1.548, round:452.667)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	433.220 (rec:1.515, round:431.705)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	411.974 (rec:1.527, round:410.447)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	390.350 (rec:1.613, round:388.737)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	369.599 (rec:1.578, round:368.021)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	347.439 (rec:1.551, round:345.888)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	324.117 (rec:1.574, round:322.543)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	299.387 (rec:1.585, round:297.802)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	274.435 (rec:1.595, round:272.840)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	247.516 (rec:1.552, round:245.964)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	219.330 (rec:1.609, round:217.722)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	188.806 (rec:1.616, round:187.190)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	157.612 (rec:1.663, round:155.948)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	125.236 (rec:1.576, round:123.660)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	94.226 (rec:1.640, round:92.586)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	63.814 (rec:1.698, round:62.115)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	37.559 (rec:1.670, round:35.890)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	18.268 (rec:1.634, round:16.635)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	7.030 (rec:1.642, round:5.388)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2.762 (rec:1.615, round:1.147)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.784 (rec:1.693, round:0.091)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1.658 (rec:1.658, round:0.000)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	1.642 (rec:1.642, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer3.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_conv1, layer3_0_bn1, layer3_0_relu1, layer3_0_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer3_0_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.0001018047332764 ch_axis=-1 pot=False)
  )
  (layer3): Module(
    (0): Module(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer2_1_relu2_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_1_relu2_post_act_fake_quantizer);  layer2_1_relu2_post_act_fake_quantizer = None
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_relu1 = getattr(self.layer3, "0").relu1(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu1_post_act_fake_quantizer = self.layer3_0_relu1_post_act_fake_quantizer(layer3_0_relu1);  layer3_0_relu1 = None
    return layer3_0_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.758 (rec:0.758, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.706 (rec:0.706, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.696 (rec:0.696, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.692 (rec:0.692, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.658 (rec:0.658, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.662 (rec:0.662, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.674 (rec:0.674, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2645.364 (rec:0.644, round:2644.720)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1353.934 (rec:0.667, round:1353.267)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1247.995 (rec:0.658, round:1247.337)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1178.544 (rec:0.653, round:1177.891)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1119.432 (rec:0.632, round:1118.800)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1064.859 (rec:0.639, round:1064.220)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1014.144 (rec:0.642, round:1013.503)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	965.783 (rec:0.652, round:965.131)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	918.870 (rec:0.633, round:918.237)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	873.984 (rec:0.632, round:873.352)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	829.770 (rec:0.664, round:829.105)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	786.627 (rec:0.647, round:785.980)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	743.182 (rec:0.666, round:742.516)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	700.506 (rec:0.657, round:699.849)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	657.374 (rec:0.647, round:656.726)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	613.083 (rec:0.670, round:612.413)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	569.289 (rec:0.661, round:568.628)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	522.115 (rec:0.638, round:521.477)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	473.496 (rec:0.661, round:472.835)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	425.032 (rec:0.668, round:424.364)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	375.393 (rec:0.651, round:374.742)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	323.901 (rec:0.689, round:323.212)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	270.626 (rec:0.662, round:269.964)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	217.146 (rec:0.661, round:216.485)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	164.564 (rec:0.658, round:163.906)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	114.058 (rec:0.659, round:113.398)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	66.117 (rec:0.672, round:65.446)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	27.715 (rec:0.695, round:27.020)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	7.162 (rec:0.670, round:6.492)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.611 (rec:0.687, round:0.924)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.795 (rec:0.686, round:0.108)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.708 (rec:0.700, round:0.008)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.695 (rec:0.695, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer3.0.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_conv2, layer3_0_bn2]
[MQBENCH] INFO: GraphModule(
  (layer3): Module(
    (0): Module(
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer3_0_relu1_post_act_fake_quantizer):
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu1_post_act_fake_quantizer);  layer3_0_relu1_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    return layer3_0_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2.668 (rec:2.668, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2.466 (rec:2.466, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2.454 (rec:2.454, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2.267 (rec:2.267, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2.189 (rec:2.189, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2.123 (rec:2.123, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2.152 (rec:2.152, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	5335.836 (rec:2.058, round:5333.779)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2909.679 (rec:2.065, round:2907.614)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2684.808 (rec:2.093, round:2682.715)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2533.730 (rec:2.065, round:2531.665)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2401.266 (rec:2.175, round:2399.090)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2277.098 (rec:2.107, round:2274.991)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2162.409 (rec:2.033, round:2160.376)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2053.617 (rec:2.021, round:2051.595)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1950.161 (rec:2.077, round:1948.083)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1849.178 (rec:2.031, round:1847.147)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1751.684 (rec:2.102, round:1749.582)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1658.361 (rec:2.104, round:1656.256)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1565.820 (rec:2.105, round:1563.715)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1474.440 (rec:2.064, round:1472.376)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1382.345 (rec:2.105, round:1380.240)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1290.601 (rec:2.049, round:1288.552)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1197.852 (rec:2.128, round:1195.724)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1104.118 (rec:2.125, round:1101.993)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1008.831 (rec:2.064, round:1006.767)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	910.192 (rec:2.055, round:908.138)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	810.470 (rec:2.149, round:808.321)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	709.235 (rec:2.108, round:707.127)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	604.127 (rec:2.117, round:602.009)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	497.443 (rec:2.175, round:495.267)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	388.534 (rec:2.140, round:386.394)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	280.982 (rec:2.207, round:278.775)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	180.969 (rec:2.168, round:178.801)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	93.155 (rec:2.205, round:90.950)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	31.727 (rec:2.167, round:29.560)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	7.507 (rec:2.201, round:5.306)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2.707 (rec:2.257, round:0.450)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2.264 (rec:2.241, round:0.023)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2.205 (rec:2.205, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer3.0.downsample.0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_downsample_0, layer3_0_downsample_1]
[MQBENCH] INFO: GraphModule(
  (layer3): Module(
    (0): Module(
      (downsample): Module(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer2_1_relu2_post_act_fake_quantizer):
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_1_relu2_post_act_fake_quantizer);  layer2_1_relu2_post_act_fake_quantizer = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    return layer3_0_downsample_1
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.247 (rec:0.247, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.238 (rec:0.238, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.233 (rec:0.233, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.228 (rec:0.228, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.228 (rec:0.228, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.232 (rec:0.232, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.226 (rec:0.226, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	284.433 (rec:0.229, round:284.203)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	149.775 (rec:0.233, round:149.542)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	138.231 (rec:0.229, round:138.001)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	131.112 (rec:0.245, round:130.866)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	125.354 (rec:0.234, round:125.120)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	120.603 (rec:0.232, round:120.371)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	116.522 (rec:0.236, round:116.286)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	112.447 (rec:0.248, round:112.199)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	108.811 (rec:0.234, round:108.576)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	104.790 (rec:0.240, round:104.551)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	100.876 (rec:0.236, round:100.640)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	96.665 (rec:0.233, round:96.431)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	92.464 (rec:0.237, round:92.227)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	88.000 (rec:0.246, round:87.754)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	83.617 (rec:0.244, round:83.373)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	78.726 (rec:0.244, round:78.482)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	73.576 (rec:0.245, round:73.331)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	67.856 (rec:0.240, round:67.616)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	61.678 (rec:0.246, round:61.432)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	55.304 (rec:0.254, round:55.049)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	48.453 (rec:0.253, round:48.200)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	41.306 (rec:0.260, round:41.046)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	33.316 (rec:0.259, round:33.056)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	25.732 (rec:0.262, round:25.470)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	18.472 (rec:0.261, round:18.211)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	11.684 (rec:0.270, round:11.414)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	6.198 (rec:0.279, round:5.919)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	2.739 (rec:0.276, round:2.463)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	1.028 (rec:0.278, round:0.749)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.468 (rec:0.282, round:0.186)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.334 (rec:0.284, round:0.050)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.280 (rec:0.277, round:0.003)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.277 (rec:0.277, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer3.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_conv1, layer3_1_bn1, layer3_1_relu1, layer3_1_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer3_1_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0577], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8649269342422485 ch_axis=-1 pot=False)
  )
  (layer3): Module(
    (1): Module(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer3_0_relu2_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu2_post_act_fake_quantizer);  layer3_0_relu2_post_act_fake_quantizer = None
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu1 = getattr(self.layer3, "1").relu1(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu1_post_act_fake_quantizer = self.layer3_1_relu1_post_act_fake_quantizer(layer3_1_relu1);  layer3_1_relu1 = None
    return layer3_1_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.326 (rec:0.326, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.295 (rec:0.295, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.287 (rec:0.287, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.287 (rec:0.287, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.263 (rec:0.263, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.270 (rec:0.270, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.267 (rec:0.267, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	5429.309 (rec:0.269, round:5429.040)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2766.322 (rec:0.266, round:2766.056)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2551.601 (rec:0.266, round:2551.334)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2408.436 (rec:0.257, round:2408.179)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2284.265 (rec:0.258, round:2284.006)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2168.350 (rec:0.253, round:2168.097)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2058.276 (rec:0.257, round:2058.019)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1950.102 (rec:0.261, round:1949.842)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1842.917 (rec:0.260, round:1842.657)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1739.486 (rec:0.258, round:1739.228)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1636.867 (rec:0.246, round:1636.621)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1537.806 (rec:0.256, round:1537.550)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1440.633 (rec:0.256, round:1440.377)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1341.583 (rec:0.256, round:1341.327)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1242.150 (rec:0.250, round:1241.901)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1143.538 (rec:0.247, round:1143.291)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1042.583 (rec:0.257, round:1042.326)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	942.587 (rec:0.261, round:942.326)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	842.593 (rec:0.263, round:842.331)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	744.758 (rec:0.250, round:744.509)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	645.203 (rec:0.252, round:644.951)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	546.442 (rec:0.267, round:546.176)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	448.276 (rec:0.262, round:448.013)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	352.968 (rec:0.259, round:352.709)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	260.516 (rec:0.260, round:260.256)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	171.949 (rec:0.261, round:171.689)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	91.613 (rec:0.263, round:91.351)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	31.443 (rec:0.273, round:31.170)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	7.116 (rec:0.268, round:6.848)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.622 (rec:0.259, round:1.364)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.520 (rec:0.274, round:0.246)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.301 (rec:0.270, round:0.031)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.265 (rec:0.264, round:0.001)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer3.1.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_conv2, layer3_1_bn2]
[MQBENCH] INFO: GraphModule(
  (layer3): Module(
    (1): Module(
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer3_1_relu1_post_act_fake_quantizer):
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu1_post_act_fake_quantizer);  layer3_1_relu1_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    return layer3_1_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2.451 (rec:2.451, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	2.438 (rec:2.438, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	2.266 (rec:2.266, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	2.291 (rec:2.291, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	2.187 (rec:2.187, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	2.243 (rec:2.243, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	2.188 (rec:2.188, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	5339.109 (rec:2.184, round:5336.925)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2870.829 (rec:2.136, round:2868.693)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2658.447 (rec:2.180, round:2656.266)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2517.411 (rec:2.136, round:2515.275)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2396.429 (rec:2.171, round:2394.258)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2288.509 (rec:2.187, round:2286.322)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2186.740 (rec:2.145, round:2184.596)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2089.989 (rec:2.129, round:2087.860)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1996.096 (rec:2.127, round:1993.968)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1905.555 (rec:2.142, round:1903.414)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1815.739 (rec:2.208, round:1813.531)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1727.285 (rec:2.136, round:1725.149)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1640.271 (rec:2.095, round:1638.176)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1551.200 (rec:2.220, round:1548.980)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1462.100 (rec:2.219, round:1459.881)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1370.177 (rec:2.167, round:1368.011)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1275.713 (rec:2.231, round:1273.482)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1180.209 (rec:2.108, round:1178.101)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1081.420 (rec:2.170, round:1079.250)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	980.121 (rec:2.247, round:977.873)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	873.760 (rec:2.255, round:871.505)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	763.435 (rec:2.186, round:761.249)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	652.005 (rec:2.246, round:649.760)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	537.453 (rec:2.238, round:535.215)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	421.962 (rec:2.263, round:419.699)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	308.565 (rec:2.242, round:306.324)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	198.774 (rec:2.277, round:196.498)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	104.356 (rec:2.250, round:102.107)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	38.143 (rec:2.343, round:35.800)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	9.822 (rec:2.258, round:7.564)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3.130 (rec:2.271, round:0.860)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2.328 (rec:2.282, round:0.046)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2.293 (rec:2.291, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer4.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_0_conv1, layer4_0_bn1, layer4_0_relu1, layer4_0_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer4_0_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0593], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8888192772865295 ch_axis=-1 pot=False)
  )
  (layer4): Module(
    (0): Module(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer3_1_relu2_post_act_fake_quantizer):
    layer4_0_conv1 = getattr(self.layer4, "0").conv1(layer3_1_relu2_post_act_fake_quantizer);  layer3_1_relu2_post_act_fake_quantizer = None
    layer4_0_bn1 = getattr(self.layer4, "0").bn1(layer4_0_conv1);  layer4_0_conv1 = None
    layer4_0_relu1 = getattr(self.layer4, "0").relu1(layer4_0_bn1);  layer4_0_bn1 = None
    layer4_0_relu1_post_act_fake_quantizer = self.layer4_0_relu1_post_act_fake_quantizer(layer4_0_relu1);  layer4_0_relu1 = None
    return layer4_0_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer4_0_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.624 (rec:0.624, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.577 (rec:0.577, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.546 (rec:0.546, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.511 (rec:0.511, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.525 (rec:0.525, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.513 (rec:0.513, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.495 (rec:0.495, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	10711.234 (rec:0.502, round:10710.732)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	5404.047 (rec:0.491, round:5403.556)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4988.177 (rec:0.483, round:4987.695)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4694.485 (rec:0.487, round:4693.998)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4436.599 (rec:0.483, round:4436.116)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4194.192 (rec:0.462, round:4193.729)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3959.203 (rec:0.464, round:3958.739)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3730.585 (rec:0.473, round:3730.112)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3511.501 (rec:0.470, round:3511.032)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3296.313 (rec:0.475, round:3295.838)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3083.495 (rec:0.470, round:3083.025)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2874.995 (rec:0.453, round:2874.543)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2669.347 (rec:0.469, round:2668.877)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2466.670 (rec:0.448, round:2466.222)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2271.132 (rec:0.461, round:2270.671)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2076.105 (rec:0.481, round:2075.624)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1885.015 (rec:0.447, round:1884.568)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1699.287 (rec:0.455, round:1698.833)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1514.513 (rec:0.462, round:1514.051)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1335.061 (rec:0.464, round:1334.597)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1158.470 (rec:0.464, round:1158.006)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	985.903 (rec:0.463, round:985.440)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	819.384 (rec:0.459, round:818.925)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	655.520 (rec:0.472, round:655.048)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	498.547 (rec:0.464, round:498.083)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	348.988 (rec:0.465, round:348.522)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	204.999 (rec:0.468, round:204.531)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	85.678 (rec:0.464, round:85.214)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	22.673 (rec:0.467, round:22.206)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	4.951 (rec:0.470, round:4.481)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.174 (rec:0.477, round:0.698)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.523 (rec:0.473, round:0.051)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.469 (rec:0.468, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer4.0.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_0_conv2, layer4_0_bn2]
[MQBENCH] INFO: GraphModule(
  (layer4): Module(
    (0): Module(
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer4_0_relu1_post_act_fake_quantizer):
    layer4_0_conv2 = getattr(self.layer4, "0").conv2(layer4_0_relu1_post_act_fake_quantizer);  layer4_0_relu1_post_act_fake_quantizer = None
    layer4_0_bn2 = getattr(self.layer4, "0").bn2(layer4_0_conv2);  layer4_0_conv2 = None
    return layer4_0_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	10.532 (rec:10.532, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	9.157 (rec:9.157, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	8.779 (rec:8.779, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	8.682 (rec:8.682, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	8.529 (rec:8.529, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	8.230 (rec:8.230, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	8.253 (rec:8.253, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	21480.605 (rec:8.111, round:21472.494)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	12037.221 (rec:7.891, round:12029.330)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	11175.242 (rec:7.936, round:11167.306)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	10589.104 (rec:7.903, round:10581.202)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	10077.311 (rec:7.694, round:10069.616)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	9597.951 (rec:7.634, round:9590.317)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	9147.250 (rec:7.759, round:9139.491)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	8713.768 (rec:7.653, round:8706.114)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	8293.651 (rec:7.781, round:8285.870)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	7888.153 (rec:7.893, round:7880.260)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	7486.791 (rec:7.781, round:7479.010)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	7092.778 (rec:7.558, round:7085.220)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	6701.777 (rec:7.428, round:6694.349)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	6311.944 (rec:7.728, round:6304.217)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	5930.548 (rec:7.722, round:5922.826)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	5549.181 (rec:7.948, round:5541.233)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	5166.365 (rec:7.721, round:5158.644)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	4781.272 (rec:7.749, round:4773.523)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	4391.869 (rec:7.990, round:4383.879)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	3997.886 (rec:7.740, round:3990.146)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	3598.120 (rec:7.747, round:3590.372)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	3194.867 (rec:7.753, round:3187.115)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	2787.025 (rec:7.804, round:2779.220)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2370.793 (rec:7.701, round:2363.092)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	1957.669 (rec:7.718, round:1949.950)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1546.035 (rec:7.896, round:1538.139)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1138.092 (rec:7.925, round:1130.167)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	748.157 (rec:7.965, round:740.192)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	390.665 (rec:8.151, round:382.514)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	125.135 (rec:8.098, round:117.037)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	22.847 (rec:8.046, round:14.801)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	8.722 (rec:8.048, round:0.674)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	8.246 (rec:8.239, round:0.007)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer4.0.downsample.0
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_0_downsample_0, layer4_0_downsample_1]
[MQBENCH] INFO: GraphModule(
  (layer4): Module(
    (0): Module(
      (downsample): Module(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer3_1_relu2_post_act_fake_quantizer):
    layer4_0_downsample_0 = getattr(getattr(self.layer4, "0").downsample, "0")(layer3_1_relu2_post_act_fake_quantizer);  layer3_1_relu2_post_act_fake_quantizer = None
    layer4_0_downsample_1 = getattr(getattr(self.layer4, "0").downsample, "1")(layer4_0_downsample_0);  layer4_0_downsample_0 = None
    return layer4_0_downsample_1
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	2.142 (rec:2.142, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1.968 (rec:1.968, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	1.922 (rec:1.922, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	1.895 (rec:1.895, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	1.944 (rec:1.944, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	1.839 (rec:1.839, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	1.846 (rec:1.846, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	1151.294 (rec:1.863, round:1149.431)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	660.610 (rec:1.894, round:658.716)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	615.440 (rec:1.928, round:613.511)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	586.458 (rec:1.856, round:584.601)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	562.420 (rec:1.870, round:560.550)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	541.679 (rec:1.929, round:539.750)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	522.551 (rec:1.935, round:520.616)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	505.048 (rec:1.850, round:503.198)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	487.446 (rec:1.896, round:485.550)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	470.430 (rec:1.908, round:468.523)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	453.693 (rec:1.888, round:451.805)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	437.273 (rec:1.892, round:435.381)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	419.765 (rec:1.924, round:417.841)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	402.204 (rec:1.933, round:400.271)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	384.565 (rec:1.881, round:382.684)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	365.727 (rec:1.882, round:363.845)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	346.341 (rec:1.945, round:344.397)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	324.874 (rec:1.926, round:322.948)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	302.465 (rec:1.905, round:300.561)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	280.016 (rec:1.959, round:278.057)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	254.829 (rec:1.938, round:252.891)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	228.838 (rec:1.991, round:226.847)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	200.679 (rec:2.006, round:198.672)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	172.176 (rec:1.996, round:170.180)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	141.326 (rec:2.024, round:139.302)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	109.775 (rec:2.059, round:107.716)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	77.100 (rec:2.120, round:74.980)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	44.119 (rec:2.062, round:42.058)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	19.146 (rec:2.149, round:16.997)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	6.126 (rec:2.151, round:3.976)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2.734 (rec:2.153, round:0.581)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	2.151 (rec:2.106, round:0.045)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	2.205 (rec:2.203, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer4.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_1_conv1, layer4_1_bn1, layer4_1_relu1, layer4_1_relu1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer4_1_relu1_post_act_fake_quantizer): FixedFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.7866079211235046 ch_axis=-1 pot=False)
  )
  (layer4): Module(
    (1): Module(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
    )
  )
)

def forward(self, layer4_0_relu2_post_act_fake_quantizer):
    layer4_1_conv1 = getattr(self.layer4, "1").conv1(layer4_0_relu2_post_act_fake_quantizer);  layer4_0_relu2_post_act_fake_quantizer = None
    layer4_1_bn1 = getattr(self.layer4, "1").bn1(layer4_1_conv1);  layer4_1_conv1 = None
    layer4_1_relu1 = getattr(self.layer4, "1").relu1(layer4_1_bn1);  layer4_1_bn1 = None
    layer4_1_relu1_post_act_fake_quantizer = self.layer4_1_relu1_post_act_fake_quantizer(layer4_1_relu1);  layer4_1_relu1 = None
    return layer4_1_relu1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer4_1_relu1_post_act_fake_quantizer
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.805 (rec:0.805, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.665 (rec:0.665, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.588 (rec:0.588, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.586 (rec:0.586, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.564 (rec:0.564, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.570 (rec:0.570, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.556 (rec:0.556, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	21344.043 (rec:0.509, round:21343.535)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	10587.279 (rec:0.528, round:10586.751)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	9777.337 (rec:0.491, round:9776.846)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	9195.932 (rec:0.515, round:9195.417)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	8668.303 (rec:0.510, round:8667.793)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	8163.702 (rec:0.460, round:8163.241)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	7667.635 (rec:0.494, round:7667.142)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	7185.587 (rec:0.410, round:7185.177)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	6711.790 (rec:0.457, round:6711.333)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	6252.649 (rec:0.469, round:6252.180)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	5805.153 (rec:0.481, round:5804.673)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	5369.414 (rec:0.468, round:5368.945)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	4946.837 (rec:0.484, round:4946.353)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	4538.002 (rec:0.484, round:4537.519)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	4147.140 (rec:0.484, round:4146.656)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	3767.317 (rec:0.471, round:3766.846)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	3399.202 (rec:0.456, round:3398.746)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	3036.880 (rec:0.477, round:3036.403)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	2691.416 (rec:0.474, round:2690.941)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	2351.061 (rec:0.487, round:2350.574)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	2026.151 (rec:0.455, round:2025.696)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1715.379 (rec:0.456, round:1714.923)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1416.159 (rec:0.433, round:1415.727)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	1132.776 (rec:0.486, round:1132.290)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	865.159 (rec:0.468, round:864.692)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	612.838 (rec:0.485, round:612.352)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	389.128 (rec:0.485, round:388.643)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	203.884 (rec:0.471, round:203.413)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	76.820 (rec:0.489, round:76.331)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	17.993 (rec:0.471, round:17.521)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2.612 (rec:0.491, round:2.122)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.579 (rec:0.471, round:0.108)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.493 (rec:0.491, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for layer4.1.conv2
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_1_conv2, layer4_1_bn2]
[MQBENCH] INFO: GraphModule(
  (layer4): Module(
    (1): Module(
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
)

def forward(self, layer4_1_relu1_post_act_fake_quantizer):
    layer4_1_conv2 = getattr(self.layer4, "1").conv2(layer4_1_relu1_post_act_fake_quantizer);  layer4_1_relu1_post_act_fake_quantizer = None
    layer4_1_bn2 = getattr(self.layer4, "1").bn2(layer4_1_conv2);  layer4_1_conv2 = None
    return layer4_1_bn2
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	399.345 (rec:399.345, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	408.440 (rec:408.440, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	371.212 (rec:371.212, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	366.260 (rec:366.260, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	369.004 (rec:369.004, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	360.068 (rec:360.068, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	330.386 (rec:330.386, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	21807.312 (rec:341.445, round:21465.867)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	15066.130 (rec:338.128, round:14728.002)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	14179.712 (rec:331.127, round:13848.585)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	13603.999 (rec:306.362, round:13297.637)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	13152.839 (rec:305.025, round:12847.813)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	12788.076 (rec:340.698, round:12447.379)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	12404.827 (rec:333.660, round:12071.167)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	12057.054 (rec:342.066, round:11714.987)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	11696.159 (rec:333.131, round:11363.028)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	11317.791 (rec:298.153, round:11019.638)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	11003.818 (rec:321.386, round:10682.433)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	10645.416 (rec:296.347, round:10349.068)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	10329.488 (rec:319.853, round:10009.636)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	9967.323 (rec:295.470, round:9671.853)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	9647.860 (rec:317.546, round:9330.314)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	9300.967 (rec:317.574, round:8983.393)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	8955.443 (rec:323.980, round:8631.464)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	8586.407 (rec:316.440, round:8269.968)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	8210.979 (rec:315.252, round:7895.726)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	7849.230 (rec:334.366, round:7514.865)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	7438.692 (rec:321.888, round:7116.804)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	6993.631 (rec:291.633, round:6701.998)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	6594.878 (rec:327.010, round:6267.868)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	6144.483 (rec:332.715, round:5811.768)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	5650.422 (rec:318.041, round:5332.381)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	5141.619 (rec:320.857, round:4820.761)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	4577.568 (rec:300.221, round:4277.347)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	4022.177 (rec:327.681, round:3694.496)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3386.431 (rec:316.993, round:3069.438)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2721.533 (rec:322.960, round:2398.573)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	2004.070 (rec:325.450, round:1678.620)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	1271.835 (rec:329.866, round:941.969)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	649.995 (rec:333.722, round:316.273)	b=2.00	count=20000
[MQBENCH] INFO: prepare layer reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [fc]
[MQBENCH] INFO: GraphModule(
  (fc): Linear(
    in_features=512, out_features=1000, bias=True
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
      (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
    )
  )
)

def forward(self, reshape_1_post_act_fake_quantizer):
    fc = self.fc(reshape_1_post_act_fake_quantizer);  reshape_1_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	160.667 (rec:160.667, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	148.612 (rec:148.612, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	167.293 (rec:167.293, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	136.859 (rec:136.859, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	166.559 (rec:166.559, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	136.368 (rec:136.368, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	163.742 (rec:163.742, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	4666.152 (rec:135.698, round:4530.454)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	3046.026 (rec:169.194, round:2876.832)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2815.924 (rec:154.053, round:2661.872)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2668.744 (rec:164.899, round:2503.845)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2513.840 (rec:153.496, round:2360.344)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2379.973 (rec:155.003, round:2224.970)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2264.500 (rec:168.113, round:2096.386)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2134.834 (rec:164.648, round:1970.186)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2010.917 (rec:161.939, round:1848.977)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1890.548 (rec:160.176, round:1730.372)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1749.509 (rec:134.128, round:1615.381)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1639.510 (rec:134.101, round:1505.409)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1563.113 (rec:163.662, round:1399.451)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1465.363 (rec:169.809, round:1295.554)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1346.767 (rec:152.258, round:1194.509)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1233.422 (rec:133.883, round:1099.539)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1171.758 (rec:163.335, round:1008.422)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1085.363 (rec:163.313, round:922.050)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	994.433 (rec:155.602, round:838.831)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	913.643 (rec:155.585, round:758.058)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	837.500 (rec:157.506, round:679.995)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	768.182 (rec:162.981, round:605.201)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	703.372 (rec:169.421, round:533.951)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	628.143 (rec:162.883, round:465.260)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	559.117 (rec:160.540, round:398.577)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	502.228 (rec:166.982, round:335.245)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	417.610 (rec:144.383, round:273.227)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	370.002 (rec:157.362, round:212.640)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	288.047 (rec:133.355, round:154.692)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	264.145 (rec:163.378, round:100.767)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	216.020 (rec:163.023, round:52.997)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	182.094 (rec:164.771, round:17.324)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	174.077 (rec:171.710, round:2.367)	b=2.00	count=20000
[MQBENCH] INFO: Disable observer and Enable quantize.
Test: [  0/782]	Time  5.410 ( 5.410)	Acc@1  85.94 ( 85.94)	Acc@5  92.19 ( 92.19)
Test: [100/782]	Time  2.203 ( 0.729)	Acc@1  82.81 ( 69.29)	Acc@5  95.31 ( 89.12)
Test: [200/782]	Time  0.182 ( 0.685)	Acc@1  64.06 ( 68.93)	Acc@5  90.62 ( 90.27)
Test: [300/782]	Time  2.641 ( 0.672)	Acc@1  59.38 ( 69.14)	Acc@5  92.19 ( 90.42)
Test: [400/782]	Time  0.926 ( 0.661)	Acc@1  57.81 ( 66.03)	Acc@5  85.94 ( 88.17)
Test: [500/782]	Time  2.449 ( 0.674)	Acc@1  68.75 ( 64.55)	Acc@5  90.62 ( 86.75)
Test: [600/782]	Time  2.100 ( 0.669)	Acc@1  73.44 ( 63.39)	Acc@5  89.06 ( 85.80)
Test: [700/782]	Time  2.409 ( 0.665)	Acc@1  60.94 ( 62.37)	Acc@5  85.94 ( 84.93)
 * Acc@1 62.414 Acc@5 84.992
srun: error: SH-IDC1-10-5-36-152: task 0: Segmentation fault
srun: Terminating job step 412223.0
