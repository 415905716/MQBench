[MQBENCH] INFO: Quantize model Scheme: BackendType.Academic Mode: Training
[MQBENCH] INFO: Weight Qconfig:
    FakeQuantize: AdaRoundFakeQuantize Params: {}
    Oberver:      MSEObserver Params: Symmetric: False / Bitwidth: 2 / Per channel: True / Pot scale: False / Extra kwargs: {'p': 2.4}
[MQBENCH] INFO: Activation Qconfig:
    FakeQuantize: QDropFakeQuantize Params: {}
    Oberver:      EMAMSEObserver Params: Symmetric: False / Bitwidth: 4 / Per channel: False / Pot scale: False / Extra kwargs: {'p': 2.4}
[MQBENCH] INFO: Replace module to qat module.
[MQBENCH] INFO: Set layer conv1 to 8 bit.
[MQBENCH] INFO: Set layer fc to 8 bit.
[MQBENCH] INFO: Set x post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant x_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant maxpool_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer1_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer2_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer3_1_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_0_relu2_post_act_fake_quantizer
[MQBENCH] INFO: Insert act quant layer4_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: Set reshape_1 post act quantize to 8 bit.
[MQBENCH] INFO: Insert act quant reshape_1_post_act_fake_quantizer
begin load datset
finish load datset
begin calibration now!
[MQBENCH] INFO: Enable observer and Disable quantize for act_fake_quant
[rank 0000]-[INFO]-[5d24]-[2022-02-23 11:32:43]-[/spring/src/linklink/src/core.cc:219]: linklink init: world_size=1, rank=(0,0), device_num=1, thread_pool=1, buffer_pool=4
[MQBENCH] INFO: Enable observer and Disable quantize for weight_fake_quant
begin advanced PTQ now!
[MQBENCH] INFO: Disable observer and Disable quantize.
[MQBENCH] INFO: Disable observer and Enable quantize.
[MQBENCH] INFO: prepare block reconstruction for conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [conv1, bn1, relu_1, maxpool, maxpool_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (conv1): Conv2d(
    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
      (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
    )
  )
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (maxpool_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0821], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.2313255071640015 ch_axis=-1 pot=False)
  )
)

def forward(self, x_post_act_fake_quantizer):
    conv1 = self.conv1(x_post_act_fake_quantizer);  x_post_act_fake_quantizer = None
    bn1 = self.bn1(conv1);  conv1 = None
    relu_1 = self.relu(bn1);  bn1 = None
    maxpool = self.maxpool(relu_1);  relu_1 = None
    maxpool_post_act_fake_quantizer = self.maxpool_post_act_fake_quantizer(maxpool);  maxpool = None
    return maxpool_post_act_fake_quantizer
    
Init alpha to be FP32
[MQBENCH] INFO: learn the scale for maxpool_post_act_fake_quantizer
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.014 (rec:0.014, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.016 (rec:0.016, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.014 (rec:0.014, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.015 (rec:0.015, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.016 (rec:0.016, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.014 (rec:0.014, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.016 (rec:0.016, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	88.952 (rec:0.018, round:88.934)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	39.121 (rec:0.013, round:39.108)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	36.099 (rec:0.016, round:36.083)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	34.192 (rec:0.014, round:34.179)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	32.830 (rec:0.014, round:32.816)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	31.430 (rec:0.013, round:31.417)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	30.043 (rec:0.015, round:30.028)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	28.820 (rec:0.016, round:28.805)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	27.648 (rec:0.016, round:27.632)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	26.523 (rec:0.018, round:26.506)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	25.291 (rec:0.015, round:25.276)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	23.924 (rec:0.016, round:23.908)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	22.691 (rec:0.016, round:22.676)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	21.422 (rec:0.016, round:21.406)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	19.925 (rec:0.014, round:19.911)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	18.528 (rec:0.015, round:18.513)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	16.926 (rec:0.013, round:16.913)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	15.406 (rec:0.014, round:15.391)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	13.645 (rec:0.015, round:13.630)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	11.865 (rec:0.014, round:11.851)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	10.010 (rec:0.015, round:9.995)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	8.130 (rec:0.015, round:8.115)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	6.220 (rec:0.014, round:6.205)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	4.052 (rec:0.016, round:4.036)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2.155 (rec:0.017, round:2.138)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	0.984 (rec:0.016, round:0.967)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	0.399 (rec:0.014, round:0.385)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	0.224 (rec:0.015, round:0.209)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	0.109 (rec:0.018, round:0.091)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	0.040 (rec:0.014, round:0.026)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.019 (rec:0.014, round:0.005)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.015 (rec:0.015, round:0.000)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.015 (rec:0.015, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer1.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_0_conv1, layer1_0_bn1, layer1_0_relu1, layer1_0_relu1_post_act_fake_quantizer, layer1_0_conv2, layer1_0_bn2, add_1, layer1_0_relu2, layer1_0_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer1_0_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0594], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8902527093887329 ch_axis=-1 pot=False)
  )
  (layer1_0_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1090], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.6353975534439087 ch_axis=-1 pot=False)
  )
  (layer1): Module(
    (0): Module(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
    )
  )
)

def forward(self, maxpool_post_act_fake_quantizer):
    layer1_0_conv1 = getattr(self.layer1, "0").conv1(maxpool_post_act_fake_quantizer)
    layer1_0_bn1 = getattr(self.layer1, "0").bn1(layer1_0_conv1);  layer1_0_conv1 = None
    layer1_0_relu1 = getattr(self.layer1, "0").relu1(layer1_0_bn1);  layer1_0_bn1 = None
    layer1_0_relu1_post_act_fake_quantizer = self.layer1_0_relu1_post_act_fake_quantizer(layer1_0_relu1);  layer1_0_relu1 = None
    layer1_0_conv2 = getattr(self.layer1, "0").conv2(layer1_0_relu1_post_act_fake_quantizer);  layer1_0_relu1_post_act_fake_quantizer = None
    layer1_0_bn2 = getattr(self.layer1, "0").bn2(layer1_0_conv2);  layer1_0_conv2 = None
    add_1 = layer1_0_bn2 + maxpool_post_act_fake_quantizer;  layer1_0_bn2 = maxpool_post_act_fake_quantizer = None
    layer1_0_relu2 = getattr(self.layer1, "0").relu2(add_1);  add_1 = None
    layer1_0_relu2_post_act_fake_quantizer = self.layer1_0_relu2_post_act_fake_quantizer(layer1_0_relu2);  layer1_0_relu2 = None
    return layer1_0_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_0_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.386 (rec:0.386, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.293 (rec:0.293, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.267 (rec:0.267, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.240 (rec:0.240, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.226 (rec:0.226, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.214 (rec:0.214, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.204 (rec:0.204, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	633.408 (rec:0.189, round:633.219)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	332.946 (rec:0.203, round:332.743)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	304.495 (rec:0.203, round:304.292)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	286.945 (rec:0.200, round:286.745)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	272.080 (rec:0.184, round:271.895)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	258.293 (rec:0.190, round:258.103)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	245.051 (rec:0.190, round:244.861)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	232.906 (rec:0.181, round:232.726)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	220.873 (rec:0.188, round:220.685)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	209.472 (rec:0.192, round:209.281)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	198.289 (rec:0.188, round:198.101)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	187.049 (rec:0.190, round:186.859)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	176.193 (rec:0.193, round:176.000)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	165.271 (rec:0.194, round:165.077)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	154.337 (rec:0.195, round:154.141)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	142.964 (rec:0.196, round:142.768)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	131.809 (rec:0.184, round:131.625)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	120.537 (rec:0.184, round:120.353)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	109.487 (rec:0.193, round:109.294)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	97.789 (rec:0.203, round:97.586)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	85.690 (rec:0.191, round:85.499)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	73.165 (rec:0.215, round:72.951)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	61.027 (rec:0.214, round:60.812)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	48.893 (rec:0.222, round:48.671)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	36.807 (rec:0.219, round:36.588)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	25.681 (rec:0.237, round:25.444)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	16.266 (rec:0.246, round:16.020)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	8.381 (rec:0.264, round:8.118)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3.322 (rec:0.259, round:3.063)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.030 (rec:0.279, round:0.751)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.408 (rec:0.284, round:0.124)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.279 (rec:0.272, round:0.007)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.288 (rec:0.288, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer1.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer1_1_conv1, layer1_1_bn1, layer1_1_relu1, layer1_1_relu1_post_act_fake_quantizer, layer1_1_conv2, layer1_1_bn2, add_2, layer1_1_relu2, layer1_1_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer1_1_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0625], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9378153085708618 ch_axis=-1 pot=False)
  )
  (layer1_1_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1312], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.9684065580368042 ch_axis=-1 pot=False)
  )
  (layer1): Module(
    (1): Module(
      (conv1): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
    )
  )
)

def forward(self, layer1_0_relu2_post_act_fake_quantizer):
    layer1_1_conv1 = getattr(self.layer1, "1").conv1(layer1_0_relu2_post_act_fake_quantizer)
    layer1_1_bn1 = getattr(self.layer1, "1").bn1(layer1_1_conv1);  layer1_1_conv1 = None
    layer1_1_relu1 = getattr(self.layer1, "1").relu1(layer1_1_bn1);  layer1_1_bn1 = None
    layer1_1_relu1_post_act_fake_quantizer = self.layer1_1_relu1_post_act_fake_quantizer(layer1_1_relu1);  layer1_1_relu1 = None
    layer1_1_conv2 = getattr(self.layer1, "1").conv2(layer1_1_relu1_post_act_fake_quantizer);  layer1_1_relu1_post_act_fake_quantizer = None
    layer1_1_bn2 = getattr(self.layer1, "1").bn2(layer1_1_conv2);  layer1_1_conv2 = None
    add_2 = layer1_1_bn2 + layer1_0_relu2_post_act_fake_quantizer;  layer1_1_bn2 = layer1_0_relu2_post_act_fake_quantizer = None
    layer1_1_relu2 = getattr(self.layer1, "1").relu2(add_2);  add_2 = None
    layer1_1_relu2_post_act_fake_quantizer = self.layer1_1_relu2_post_act_fake_quantizer(layer1_1_relu2);  layer1_1_relu2 = None
    return layer1_1_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer1_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer1_1_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.728 (rec:0.728, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.595 (rec:0.595, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.558 (rec:0.558, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.544 (rec:0.544, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.527 (rec:0.527, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.532 (rec:0.532, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.477 (rec:0.477, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	670.433 (rec:0.468, round:669.965)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	396.874 (rec:0.482, round:396.392)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	367.678 (rec:0.486, round:367.191)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	347.801 (rec:0.509, round:347.292)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	331.013 (rec:0.486, round:330.528)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	315.506 (rec:0.488, round:315.018)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	300.523 (rec:0.469, round:300.054)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	285.694 (rec:0.488, round:285.206)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	271.448 (rec:0.480, round:270.967)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	256.938 (rec:0.490, round:256.448)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	243.816 (rec:0.490, round:243.326)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	230.894 (rec:0.472, round:230.422)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	217.855 (rec:0.473, round:217.382)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	205.421 (rec:0.493, round:204.928)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	192.302 (rec:0.496, round:191.806)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	179.164 (rec:0.481, round:178.683)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	165.596 (rec:0.508, round:165.089)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	151.536 (rec:0.500, round:151.036)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	137.174 (rec:0.496, round:136.678)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	122.148 (rec:0.509, round:121.639)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	107.149 (rec:0.526, round:106.623)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	91.682 (rec:0.508, round:91.174)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	75.567 (rec:0.535, round:75.031)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	59.338 (rec:0.529, round:58.810)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	43.493 (rec:0.543, round:42.949)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	29.249 (rec:0.544, round:28.704)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	17.943 (rec:0.570, round:17.373)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	9.360 (rec:0.559, round:8.801)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	3.941 (rec:0.579, round:3.363)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.436 (rec:0.594, round:0.842)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.696 (rec:0.595, round:0.101)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.578 (rec:0.575, round:0.003)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.594 (rec:0.594, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer2.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_0_conv1, layer2_0_downsample_0, layer2_0_bn1, layer2_0_downsample_1, layer2_0_relu1, layer2_0_relu1_post_act_fake_quantizer, layer2_0_conv2, layer2_0_bn2, add_3, layer2_0_relu2, layer2_0_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer2_0_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0611], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9161999225616455 ch_axis=-1 pot=False)
  )
  (layer2_0_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0886], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.3294724225997925 ch_axis=-1 pot=False)
  )
  (layer2): Module(
    (0): Module(
      (conv1): Conv2d(
        64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (downsample): Module(
        (0): Conv2d(
          64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer1_1_relu2_post_act_fake_quantizer):
    layer2_0_conv1 = getattr(self.layer2, "0").conv1(layer1_1_relu2_post_act_fake_quantizer)
    layer2_0_downsample_0 = getattr(getattr(self.layer2, "0").downsample, "0")(layer1_1_relu2_post_act_fake_quantizer);  layer1_1_relu2_post_act_fake_quantizer = None
    layer2_0_bn1 = getattr(self.layer2, "0").bn1(layer2_0_conv1);  layer2_0_conv1 = None
    layer2_0_downsample_1 = getattr(getattr(self.layer2, "0").downsample, "1")(layer2_0_downsample_0);  layer2_0_downsample_0 = None
    layer2_0_relu1 = getattr(self.layer2, "0").relu1(layer2_0_bn1);  layer2_0_bn1 = None
    layer2_0_relu1_post_act_fake_quantizer = self.layer2_0_relu1_post_act_fake_quantizer(layer2_0_relu1);  layer2_0_relu1 = None
    layer2_0_conv2 = getattr(self.layer2, "0").conv2(layer2_0_relu1_post_act_fake_quantizer);  layer2_0_relu1_post_act_fake_quantizer = None
    layer2_0_bn2 = getattr(self.layer2, "0").bn2(layer2_0_conv2);  layer2_0_conv2 = None
    add_3 = layer2_0_bn2 + layer2_0_downsample_1;  layer2_0_bn2 = layer2_0_downsample_1 = None
    layer2_0_relu2 = getattr(self.layer2, "0").relu2(add_3);  add_3 = None
    layer2_0_relu2_post_act_fake_quantizer = self.layer2_0_relu2_post_act_fake_quantizer(layer2_0_relu2);  layer2_0_relu2 = None
    return layer2_0_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_0_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.425 (rec:0.425, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.359 (rec:0.359, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.353 (rec:0.353, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.333 (rec:0.333, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.299 (rec:0.299, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.310 (rec:0.310, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.291 (rec:0.291, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2061.618 (rec:0.277, round:2061.341)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1076.709 (rec:0.299, round:1076.410)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	984.481 (rec:0.305, round:984.175)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	925.155 (rec:0.300, round:924.855)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	873.664 (rec:0.295, round:873.369)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	827.753 (rec:0.295, round:827.458)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	785.645 (rec:0.287, round:785.358)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	745.359 (rec:0.297, round:745.062)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	706.257 (rec:0.299, round:705.958)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	667.607 (rec:0.308, round:667.299)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	630.382 (rec:0.311, round:630.071)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	594.447 (rec:0.310, round:594.138)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	559.141 (rec:0.312, round:558.828)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	522.760 (rec:0.318, round:522.442)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	486.875 (rec:0.317, round:486.558)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	450.196 (rec:0.316, round:449.879)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	411.184 (rec:0.321, round:410.863)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	372.066 (rec:0.329, round:371.737)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	332.569 (rec:0.335, round:332.234)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	291.403 (rec:0.345, round:291.059)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	249.328 (rec:0.339, round:248.989)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	207.173 (rec:0.349, round:206.824)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	164.797 (rec:0.356, round:164.441)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	125.310 (rec:0.377, round:124.933)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	87.890 (rec:0.374, round:87.517)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	55.509 (rec:0.383, round:55.125)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	30.364 (rec:0.394, round:29.971)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	13.091 (rec:0.417, round:12.674)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4.089 (rec:0.426, round:3.663)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.117 (rec:0.425, round:0.693)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.531 (rec:0.427, round:0.104)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.463 (rec:0.440, round:0.024)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.447 (rec:0.445, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer2.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer2_1_conv1, layer2_1_bn1, layer2_1_relu1, layer2_1_relu1_post_act_fake_quantizer, layer2_1_conv2, layer2_1_bn2, add_4, layer2_1_relu2, layer2_1_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer2_1_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0651], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.9771558046340942 ch_axis=-1 pot=False)
  )
  (layer2_1_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1111], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.66663658618927 ch_axis=-1 pot=False)
  )
  (layer2): Module(
    (1): Module(
      (conv1): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
    )
  )
)

def forward(self, layer2_0_relu2_post_act_fake_quantizer):
    layer2_1_conv1 = getattr(self.layer2, "1").conv1(layer2_0_relu2_post_act_fake_quantizer)
    layer2_1_bn1 = getattr(self.layer2, "1").bn1(layer2_1_conv1);  layer2_1_conv1 = None
    layer2_1_relu1 = getattr(self.layer2, "1").relu1(layer2_1_bn1);  layer2_1_bn1 = None
    layer2_1_relu1_post_act_fake_quantizer = self.layer2_1_relu1_post_act_fake_quantizer(layer2_1_relu1);  layer2_1_relu1 = None
    layer2_1_conv2 = getattr(self.layer2, "1").conv2(layer2_1_relu1_post_act_fake_quantizer);  layer2_1_relu1_post_act_fake_quantizer = None
    layer2_1_bn2 = getattr(self.layer2, "1").bn2(layer2_1_conv2);  layer2_1_conv2 = None
    add_4 = layer2_1_bn2 + layer2_0_relu2_post_act_fake_quantizer;  layer2_1_bn2 = layer2_0_relu2_post_act_fake_quantizer = None
    layer2_1_relu2 = getattr(self.layer2, "1").relu2(add_4);  add_4 = None
    layer2_1_relu2_post_act_fake_quantizer = self.layer2_1_relu2_post_act_fake_quantizer(layer2_1_relu2);  layer2_1_relu2 = None
    return layer2_1_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer2_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer2_1_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.690 (rec:0.690, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.615 (rec:0.615, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.580 (rec:0.580, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.566 (rec:0.566, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.556 (rec:0.556, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.540 (rec:0.540, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.524 (rec:0.524, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	2672.224 (rec:0.538, round:2671.686)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	1422.248 (rec:0.532, round:1421.716)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	1309.555 (rec:0.567, round:1308.989)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	1235.649 (rec:0.545, round:1235.104)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	1172.126 (rec:0.544, round:1171.582)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	1114.821 (rec:0.544, round:1114.277)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	1060.024 (rec:0.532, round:1059.492)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	1007.548 (rec:0.537, round:1007.011)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	956.313 (rec:0.534, round:955.779)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	907.557 (rec:0.543, round:907.014)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	858.581 (rec:0.549, round:858.032)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	811.760 (rec:0.547, round:811.213)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	765.786 (rec:0.536, round:765.250)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	718.877 (rec:0.558, round:718.319)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	670.410 (rec:0.548, round:669.862)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	621.880 (rec:0.545, round:621.334)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	571.543 (rec:0.546, round:570.998)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	521.199 (rec:0.557, round:520.643)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	469.720 (rec:0.569, round:469.151)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	416.437 (rec:0.594, round:415.842)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	363.150 (rec:0.563, round:362.587)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	307.993 (rec:0.583, round:307.410)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	249.255 (rec:0.593, round:248.662)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	192.659 (rec:0.594, round:192.064)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	138.264 (rec:0.589, round:137.674)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	87.896 (rec:0.611, round:87.285)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	48.116 (rec:0.623, round:47.494)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	19.888 (rec:0.607, round:19.281)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	5.451 (rec:0.609, round:4.842)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	1.394 (rec:0.627, round:0.767)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.725 (rec:0.634, round:0.090)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.629 (rec:0.627, round:0.002)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.627 (rec:0.627, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer3.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_0_conv1, layer3_0_downsample_0, layer3_0_bn1, layer3_0_downsample_1, layer3_0_relu1, layer3_0_relu1_post_act_fake_quantizer, layer3_0_conv2, layer3_0_bn2, add_5, layer3_0_relu2, layer3_0_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer3_0_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0667], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.0001018047332764 ch_axis=-1 pot=False)
  )
  (layer3_0_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0799], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.1985894441604614 ch_axis=-1 pot=False)
  )
  (layer3): Module(
    (0): Module(
      (conv1): Conv2d(
        128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (downsample): Module(
        (0): Conv2d(
          128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer2_1_relu2_post_act_fake_quantizer):
    layer3_0_conv1 = getattr(self.layer3, "0").conv1(layer2_1_relu2_post_act_fake_quantizer)
    layer3_0_downsample_0 = getattr(getattr(self.layer3, "0").downsample, "0")(layer2_1_relu2_post_act_fake_quantizer);  layer2_1_relu2_post_act_fake_quantizer = None
    layer3_0_bn1 = getattr(self.layer3, "0").bn1(layer3_0_conv1);  layer3_0_conv1 = None
    layer3_0_downsample_1 = getattr(getattr(self.layer3, "0").downsample, "1")(layer3_0_downsample_0);  layer3_0_downsample_0 = None
    layer3_0_relu1 = getattr(self.layer3, "0").relu1(layer3_0_bn1);  layer3_0_bn1 = None
    layer3_0_relu1_post_act_fake_quantizer = self.layer3_0_relu1_post_act_fake_quantizer(layer3_0_relu1);  layer3_0_relu1 = None
    layer3_0_conv2 = getattr(self.layer3, "0").conv2(layer3_0_relu1_post_act_fake_quantizer);  layer3_0_relu1_post_act_fake_quantizer = None
    layer3_0_bn2 = getattr(self.layer3, "0").bn2(layer3_0_conv2);  layer3_0_conv2 = None
    add_5 = layer3_0_bn2 + layer3_0_downsample_1;  layer3_0_bn2 = layer3_0_downsample_1 = None
    layer3_0_relu2 = getattr(self.layer3, "0").relu2(add_5);  add_5 = None
    layer3_0_relu2_post_act_fake_quantizer = self.layer3_0_relu2_post_act_fake_quantizer(layer3_0_relu2);  layer3_0_relu2 = None
    return layer3_0_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_0_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.579 (rec:0.579, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.487 (rec:0.487, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.461 (rec:0.461, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.452 (rec:0.452, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.400 (rec:0.400, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.385 (rec:0.385, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.391 (rec:0.391, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	8388.019 (rec:0.369, round:8387.649)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	4365.127 (rec:0.396, round:4364.731)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	4032.370 (rec:0.373, round:4031.997)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	3814.515 (rec:0.385, round:3814.130)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	3628.377 (rec:0.382, round:3627.995)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	3454.378 (rec:0.384, round:3453.994)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	3290.853 (rec:0.376, round:3290.478)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	3131.144 (rec:0.377, round:3130.767)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	2975.492 (rec:0.376, round:2975.117)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	2825.721 (rec:0.368, round:2825.354)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	2674.849 (rec:0.382, round:2674.467)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	2529.518 (rec:0.399, round:2529.118)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	2382.516 (rec:0.385, round:2382.131)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2235.238 (rec:0.385, round:2234.852)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2088.536 (rec:0.393, round:2088.143)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1941.159 (rec:0.378, round:1940.781)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1791.025 (rec:0.395, round:1790.630)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1636.817 (rec:0.384, round:1636.433)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1479.835 (rec:0.410, round:1479.425)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1318.413 (rec:0.411, round:1318.002)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1153.689 (rec:0.430, round:1153.259)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	982.838 (rec:0.425, round:982.413)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	812.371 (rec:0.420, round:811.951)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	639.461 (rec:0.416, round:639.045)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	470.632 (rec:0.424, round:470.208)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	313.685 (rec:0.429, round:313.256)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	176.596 (rec:0.447, round:176.149)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	72.992 (rec:0.465, round:72.527)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	17.367 (rec:0.455, round:16.912)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	3.105 (rec:0.471, round:2.634)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	0.842 (rec:0.472, round:0.370)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.480 (rec:0.457, round:0.023)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.477 (rec:0.477, round:0.000)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer3.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer3_1_conv1, layer3_1_bn1, layer3_1_relu1, layer3_1_relu1_post_act_fake_quantizer, layer3_1_conv2, layer3_1_bn2, add_6, layer3_1_relu2, layer3_1_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer3_1_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0577], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8649269342422485 ch_axis=-1 pot=False)
  )
  (layer3_1_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0953], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.428949236869812 ch_axis=-1 pot=False)
  )
  (layer3): Module(
    (1): Module(
      (conv1): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
    )
  )
)

def forward(self, layer3_0_relu2_post_act_fake_quantizer):
    layer3_1_conv1 = getattr(self.layer3, "1").conv1(layer3_0_relu2_post_act_fake_quantizer)
    layer3_1_bn1 = getattr(self.layer3, "1").bn1(layer3_1_conv1);  layer3_1_conv1 = None
    layer3_1_relu1 = getattr(self.layer3, "1").relu1(layer3_1_bn1);  layer3_1_bn1 = None
    layer3_1_relu1_post_act_fake_quantizer = self.layer3_1_relu1_post_act_fake_quantizer(layer3_1_relu1);  layer3_1_relu1 = None
    layer3_1_conv2 = getattr(self.layer3, "1").conv2(layer3_1_relu1_post_act_fake_quantizer);  layer3_1_relu1_post_act_fake_quantizer = None
    layer3_1_bn2 = getattr(self.layer3, "1").bn2(layer3_1_conv2);  layer3_1_conv2 = None
    add_6 = layer3_1_bn2 + layer3_0_relu2_post_act_fake_quantizer;  layer3_1_bn2 = layer3_0_relu2_post_act_fake_quantizer = None
    layer3_1_relu2 = getattr(self.layer3, "1").relu2(add_6);  add_6 = None
    layer3_1_relu2_post_act_fake_quantizer = self.layer3_1_relu2_post_act_fake_quantizer(layer3_1_relu2);  layer3_1_relu2 = None
    return layer3_1_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer3_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer3_1_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	0.723 (rec:0.723, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	0.641 (rec:0.641, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.587 (rec:0.587, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.582 (rec:0.582, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.569 (rec:0.569, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.556 (rec:0.556, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.564 (rec:0.564, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	10838.656 (rec:0.531, round:10838.125)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	5690.875 (rec:0.543, round:5690.333)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	5272.506 (rec:0.552, round:5271.954)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	4998.656 (rec:0.540, round:4998.116)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	4766.714 (rec:0.545, round:4766.168)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	4549.639 (rec:0.533, round:4549.106)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	4342.336 (rec:0.528, round:4341.808)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	4140.557 (rec:0.539, round:4140.018)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	3941.729 (rec:0.521, round:3941.208)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	3746.483 (rec:0.531, round:3745.953)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	3553.150 (rec:0.526, round:3552.624)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	3360.346 (rec:0.509, round:3359.837)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	3168.843 (rec:0.529, round:3168.314)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	2975.652 (rec:0.534, round:2975.119)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	2782.370 (rec:0.533, round:2781.837)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	2587.166 (rec:0.550, round:2586.615)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	2387.216 (rec:0.541, round:2386.675)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	2184.603 (rec:0.556, round:2184.046)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	1978.425 (rec:0.541, round:1977.884)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	1768.987 (rec:0.563, round:1768.424)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	1555.676 (rec:0.527, round:1555.149)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	1340.196 (rec:0.563, round:1339.634)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	1121.787 (rec:0.564, round:1121.223)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	902.822 (rec:0.552, round:902.270)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	689.182 (rec:0.549, round:688.633)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	481.130 (rec:0.575, round:480.555)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	290.253 (rec:0.581, round:289.672)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	130.796 (rec:0.558, round:130.238)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	32.775 (rec:0.577, round:32.198)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	5.277 (rec:0.596, round:4.681)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1.207 (rec:0.584, round:0.623)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.624 (rec:0.564, round:0.060)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.591 (rec:0.591, round:0.001)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer4.0.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_0_conv1, layer4_0_downsample_0, layer4_0_bn1, layer4_0_downsample_1, layer4_0_relu1, layer4_0_relu1_post_act_fake_quantizer, layer4_0_conv2, layer4_0_bn2, add_7, layer4_0_relu2, layer4_0_relu2_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer4_0_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0593], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.8888192772865295 ch_axis=-1 pot=False)
  )
  (layer4_0_relu2_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1101], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=1.650946855545044 ch_axis=-1 pot=False)
  )
  (layer4): Module(
    (0): Module(
      (conv1): Conv2d(
        256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
      (downsample): Module(
        (0): Conv2d(
          256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False
          (weight_fake_quant): AdaRoundFakeQuantize(
            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
            (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
          )
        )
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)

def forward(self, layer3_1_relu2_post_act_fake_quantizer):
    layer4_0_conv1 = getattr(self.layer4, "0").conv1(layer3_1_relu2_post_act_fake_quantizer)
    layer4_0_downsample_0 = getattr(getattr(self.layer4, "0").downsample, "0")(layer3_1_relu2_post_act_fake_quantizer);  layer3_1_relu2_post_act_fake_quantizer = None
    layer4_0_bn1 = getattr(self.layer4, "0").bn1(layer4_0_conv1);  layer4_0_conv1 = None
    layer4_0_downsample_1 = getattr(getattr(self.layer4, "0").downsample, "1")(layer4_0_downsample_0);  layer4_0_downsample_0 = None
    layer4_0_relu1 = getattr(self.layer4, "0").relu1(layer4_0_bn1);  layer4_0_bn1 = None
    layer4_0_relu1_post_act_fake_quantizer = self.layer4_0_relu1_post_act_fake_quantizer(layer4_0_relu1);  layer4_0_relu1 = None
    layer4_0_conv2 = getattr(self.layer4, "0").conv2(layer4_0_relu1_post_act_fake_quantizer);  layer4_0_relu1_post_act_fake_quantizer = None
    layer4_0_bn2 = getattr(self.layer4, "0").bn2(layer4_0_conv2);  layer4_0_conv2 = None
    add_7 = layer4_0_bn2 + layer4_0_downsample_1;  layer4_0_bn2 = layer4_0_downsample_1 = None
    layer4_0_relu2 = getattr(self.layer4, "0").relu2(add_7);  add_7 = None
    layer4_0_relu2_post_act_fake_quantizer = self.layer4_0_relu2_post_act_fake_quantizer(layer4_0_relu2);  layer4_0_relu2 = None
    return layer4_0_relu2_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer4_0_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for layer4_0_relu2_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	1.259 (rec:1.259, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	1.069 (rec:1.069, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	0.913 (rec:0.913, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	0.921 (rec:0.921, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	0.840 (rec:0.840, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	0.781 (rec:0.781, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	0.726 (rec:0.726, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	33852.641 (rec:0.724, round:33851.918)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	17980.891 (rec:0.728, round:17980.162)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	16698.551 (rec:0.725, round:16697.826)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	15847.292 (rec:0.685, round:15846.606)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	15114.313 (rec:0.657, round:15113.657)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	14427.400 (rec:0.650, round:14426.750)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	13762.711 (rec:0.638, round:13762.073)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	13111.642 (rec:0.651, round:13110.990)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	12468.933 (rec:0.650, round:12468.282)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	11831.135 (rec:0.644, round:11830.491)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	11200.529 (rec:0.630, round:11199.899)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	10564.549 (rec:0.612, round:10563.937)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	9933.716 (rec:0.636, round:9933.079)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	9301.139 (rec:0.657, round:9300.482)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	8667.604 (rec:0.618, round:8666.985)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	8028.624 (rec:0.615, round:8028.008)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	7391.322 (rec:0.653, round:7390.669)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	6750.273 (rec:0.645, round:6749.628)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	6106.285 (rec:0.616, round:6105.668)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	5458.833 (rec:0.624, round:5458.209)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	4812.600 (rec:0.621, round:4811.979)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	4169.103 (rec:0.675, round:4168.429)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	3524.332 (rec:0.667, round:3523.665)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	2880.000 (rec:0.662, round:2879.338)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	2256.468 (rec:0.666, round:2255.802)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	1656.872 (rec:0.694, round:1656.178)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	1089.120 (rec:0.678, round:1088.442)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	576.075 (rec:0.733, round:575.342)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	175.212 (rec:0.722, round:174.490)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	24.623 (rec:0.770, round:23.853)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	3.226 (rec:0.730, round:2.496)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	0.879 (rec:0.728, round:0.152)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	0.768 (rec:0.766, round:0.002)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for layer4.1.conv1
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [layer4_1_conv1, layer4_1_bn1, layer4_1_relu1, layer4_1_relu1_post_act_fake_quantizer, layer4_1_conv2, layer4_1_bn2, add_8, layer4_1_relu2, avgpool, size, reshape_1, reshape_1_post_act_fake_quantizer]
[MQBENCH] INFO: GraphModule(
  (layer4_1_relu1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=15, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0524], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=0.7866079211235046 ch_axis=-1 pot=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (reshape_1_post_act_fake_quantizer): QDropFakeQuantize(
    fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0330], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)
    (activation_post_process): EMAMSEObserver(min_val=0.0, max_val=8.421717643737793 ch_axis=-1 pot=False)
  )
  (layer4): Module(
    (1): Module(
      (conv1): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu1): ReLU(inplace=True)
      (conv2): Conv2d(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False
        (weight_fake_quant): AdaRoundFakeQuantize(
          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=3, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
          (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
        )
      )
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu2): ReLU(inplace=True)
    )
  )
)

def forward(self, layer4_0_relu2_post_act_fake_quantizer):
    layer4_1_conv1 = getattr(self.layer4, "1").conv1(layer4_0_relu2_post_act_fake_quantizer)
    layer4_1_bn1 = getattr(self.layer4, "1").bn1(layer4_1_conv1);  layer4_1_conv1 = None
    layer4_1_relu1 = getattr(self.layer4, "1").relu1(layer4_1_bn1);  layer4_1_bn1 = None
    layer4_1_relu1_post_act_fake_quantizer = self.layer4_1_relu1_post_act_fake_quantizer(layer4_1_relu1);  layer4_1_relu1 = None
    layer4_1_conv2 = getattr(self.layer4, "1").conv2(layer4_1_relu1_post_act_fake_quantizer);  layer4_1_relu1_post_act_fake_quantizer = None
    layer4_1_bn2 = getattr(self.layer4, "1").bn2(layer4_1_conv2);  layer4_1_conv2 = None
    add_8 = layer4_1_bn2 + layer4_0_relu2_post_act_fake_quantizer;  layer4_1_bn2 = layer4_0_relu2_post_act_fake_quantizer = None
    layer4_1_relu2 = getattr(self.layer4, "1").relu2(add_8);  add_8 = None
    avgpool = self.avgpool(layer4_1_relu2);  layer4_1_relu2 = None
    size = avgpool.size(0)
    reshape_1 = avgpool.reshape(size, -1);  avgpool = size = None
    reshape_1_post_act_fake_quantizer = self.reshape_1_post_act_fake_quantizer(reshape_1);  reshape_1 = None
    return reshape_1_post_act_fake_quantizer
    
[MQBENCH] INFO: learn the scale for layer4_1_relu1_post_act_fake_quantizer
[MQBENCH] INFO: learn the scale for reshape_1_post_act_fake_quantizer
Init alpha to be FP32
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	45.235 (rec:45.235, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	26.118 (rec:26.118, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	21.792 (rec:21.792, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	17.368 (rec:17.368, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	15.324 (rec:15.324, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	14.048 (rec:14.048, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	12.387 (rec:12.387, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	43860.719 (rec:11.205, round:43849.516)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	28661.141 (rec:10.863, round:28650.277)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	26935.951 (rec:10.195, round:26925.756)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	25875.189 (rec:8.562, round:25866.627)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	25024.012 (rec:8.251, round:25015.762)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	24266.500 (rec:8.205, round:24258.295)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	23564.369 (rec:8.046, round:23556.324)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	22883.107 (rec:7.650, round:22875.457)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	22214.764 (rec:7.673, round:22207.090)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	21550.570 (rec:6.502, round:21544.068)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	20886.568 (rec:6.629, round:20879.939)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	20212.480 (rec:6.099, round:20206.381)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	19534.807 (rec:5.728, round:19529.078)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	18851.477 (rec:5.865, round:18845.611)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	18149.506 (rec:5.816, round:18143.689)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	17423.463 (rec:5.842, round:17417.621)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	16674.328 (rec:5.406, round:16668.922)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	15897.501 (rec:4.933, round:15892.568)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	15088.041 (rec:5.281, round:15082.760)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	14249.011 (rec:5.104, round:14243.906)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	13366.357 (rec:5.197, round:13361.160)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	12432.363 (rec:4.836, round:12427.527)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	11446.733 (rec:5.263, round:11441.470)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	10407.990 (rec:4.807, round:10403.184)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	9311.966 (rec:5.202, round:9306.764)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	8152.493 (rec:4.971, round:8147.521)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	6927.087 (rec:5.635, round:6921.452)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	5638.719 (rec:5.623, round:5633.096)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	4293.260 (rec:5.724, round:4287.536)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	2914.457 (rec:5.632, round:2908.825)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	1567.576 (rec:6.041, round:1561.535)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	495.558 (rec:7.008, round:488.550)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	77.232 (rec:6.682, round:70.550)	b=2.00	count=20000
[MQBENCH] INFO: prepare block reconstruction for fc
[MQBENCH] INFO: the node list is below!
[MQBENCH] INFO: [fc]
[MQBENCH] INFO: GraphModule(
  (fc): Linear(
    in_features=512, out_features=1000, bias=True
    (weight_fake_quant): AdaRoundFakeQuantize(
      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([0], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=List, zero_point=List
      (activation_post_process): MSEObserver(min_val=List, max_val=List ch_axis=0 pot=False)
    )
  )
)

def forward(self, reshape_1_post_act_fake_quantizer):
    fc = self.fc(reshape_1_post_act_fake_quantizer);  reshape_1_post_act_fake_quantizer = None
    return fc
    
Init alpha to be FP32
[MQBENCH] INFO: The world size is 1.
[MQBENCH] INFO: start tuning by adaround
[MQBENCH] INFO: Total loss:	27.514 (rec:27.514, round:0.000)	b=20.00	count=500
[MQBENCH] INFO: Total loss:	27.327 (rec:27.327, round:0.000)	b=20.00	count=1000
[MQBENCH] INFO: Total loss:	24.579 (rec:24.579, round:0.000)	b=20.00	count=1500
[MQBENCH] INFO: Total loss:	25.754 (rec:25.754, round:0.000)	b=20.00	count=2000
[MQBENCH] INFO: Total loss:	26.032 (rec:26.032, round:0.000)	b=20.00	count=2500
[MQBENCH] INFO: Total loss:	24.652 (rec:24.652, round:0.000)	b=20.00	count=3000
[MQBENCH] INFO: Total loss:	25.298 (rec:25.298, round:0.000)	b=20.00	count=3500
[MQBENCH] INFO: Total loss:	4640.245 (rec:25.822, round:4614.423)	b=20.00	count=4000
[MQBENCH] INFO: Total loss:	2866.173 (rec:25.693, round:2840.480)	b=19.44	count=4500
[MQBENCH] INFO: Total loss:	2662.818 (rec:25.513, round:2637.305)	b=18.88	count=5000
[MQBENCH] INFO: Total loss:	2524.281 (rec:26.614, round:2497.668)	b=18.31	count=5500
[MQBENCH] INFO: Total loss:	2399.529 (rec:25.277, round:2374.251)	b=17.75	count=6000
[MQBENCH] INFO: Total loss:	2283.081 (rec:24.627, round:2258.454)	b=17.19	count=6500
[MQBENCH] INFO: Total loss:	2166.616 (rec:22.400, round:2144.217)	b=16.62	count=7000
[MQBENCH] INFO: Total loss:	2057.582 (rec:24.477, round:2033.106)	b=16.06	count=7500
[MQBENCH] INFO: Total loss:	1947.912 (rec:24.492, round:1923.420)	b=15.50	count=8000
[MQBENCH] INFO: Total loss:	1841.014 (rec:25.026, round:1815.988)	b=14.94	count=8500
[MQBENCH] INFO: Total loss:	1735.474 (rec:22.209, round:1713.265)	b=14.38	count=9000
[MQBENCH] INFO: Total loss:	1633.749 (rec:22.155, round:1611.594)	b=13.81	count=9500
[MQBENCH] INFO: Total loss:	1535.657 (rec:25.823, round:1509.834)	b=13.25	count=10000
[MQBENCH] INFO: Total loss:	1434.984 (rec:24.935, round:1410.049)	b=12.69	count=10500
[MQBENCH] INFO: Total loss:	1337.031 (rec:25.493, round:1311.537)	b=12.12	count=11000
[MQBENCH] INFO: Total loss:	1240.898 (rec:24.593, round:1216.305)	b=11.56	count=11500
[MQBENCH] INFO: Total loss:	1146.650 (rec:24.653, round:1121.996)	b=11.00	count=12000
[MQBENCH] INFO: Total loss:	1053.696 (rec:24.570, round:1029.127)	b=10.44	count=12500
[MQBENCH] INFO: Total loss:	964.711 (rec:25.656, round:939.055)	b=9.88	count=13000
[MQBENCH] INFO: Total loss:	875.310 (rec:22.799, round:852.511)	b=9.31	count=13500
[MQBENCH] INFO: Total loss:	793.496 (rec:25.722, round:767.774)	b=8.75	count=14000
[MQBENCH] INFO: Total loss:	710.578 (rec:25.436, round:685.142)	b=8.19	count=14500
[MQBENCH] INFO: Total loss:	628.350 (rec:24.398, round:603.952)	b=7.62	count=15000
[MQBENCH] INFO: Total loss:	548.935 (rec:23.401, round:525.535)	b=7.06	count=15500
[MQBENCH] INFO: Total loss:	472.174 (rec:25.155, round:447.020)	b=6.50	count=16000
[MQBENCH] INFO: Total loss:	395.166 (rec:24.802, round:370.364)	b=5.94	count=16500
[MQBENCH] INFO: Total loss:	320.716 (rec:24.999, round:295.717)	b=5.38	count=17000
[MQBENCH] INFO: Total loss:	249.115 (rec:25.355, round:223.760)	b=4.81	count=17500
[MQBENCH] INFO: Total loss:	180.809 (rec:25.555, round:155.254)	b=4.25	count=18000
[MQBENCH] INFO: Total loss:	116.602 (rec:23.642, round:92.960)	b=3.69	count=18500
[MQBENCH] INFO: Total loss:	65.490 (rec:24.788, round:40.702)	b=3.12	count=19000
[MQBENCH] INFO: Total loss:	34.131 (rec:25.349, round:8.782)	b=2.56	count=19500
[MQBENCH] INFO: Total loss:	22.780 (rec:22.158, round:0.622)	b=2.00	count=20000
[MQBENCH] INFO: Disable observer and Enable quantize.
Test: [  0/782]	Time  3.714 ( 3.714)	Acc@1  89.06 ( 89.06)	Acc@5  93.75 ( 93.75)
Test: [100/782]	Time  0.046 ( 0.505)	Acc@1  76.56 ( 71.98)	Acc@5  96.88 ( 90.07)
Test: [200/782]	Time  0.052 ( 0.486)	Acc@1  73.44 ( 70.58)	Acc@5  93.75 ( 91.12)
Test: [300/782]	Time  1.213 ( 0.487)	Acc@1  73.44 ( 71.05)	Acc@5  95.31 ( 91.22)
Test: [400/782]	Time  0.543 ( 0.484)	Acc@1  60.94 ( 67.87)	Acc@5  87.50 ( 89.22)
Test: [500/782]	Time  0.606 ( 0.485)	Acc@1  71.88 ( 66.69)	Acc@5  92.19 ( 87.81)
Test: [600/782]	Time  0.050 ( 0.524)	Acc@1  70.31 ( 65.36)	Acc@5  89.06 ( 86.87)
Test: [700/782]	Time  0.048 ( 0.566)	Acc@1  62.50 ( 64.35)	Acc@5  90.62 ( 86.11)
 * Acc@1 64.390 Acc@5 86.222
Error in system call pthread_mutex_destroy: Device or resource busy
srun: error: SH-IDC1-10-5-36-152: task 0: Segmentation fault
srun: Terminating job step 412214.0
